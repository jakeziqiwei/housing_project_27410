---
title: "STAT 27410 Final Project Proposal"
author: "Bridgette Kon and Jake Wei"
fontsize: 12pt
output:
  pdf_document: default
  html_document:
    df_print: paged
  word_document: default
  geometry: margin=0.75in
fig_crop: no
---

```{r echo=FALSE, message=FALSE}
library(knitr)
knitr::opts_chunk$set(message=F, warning=F)
options(scipen=6, digits=6)
library(tidyverse)
library(ggplot2)
library(ggcorrplot)
library(mosaic)
library(car)
library(MASS)
library(broom)
library(xtable)
library(stargazer)
```

# 1. Introduction

Airbnb has become one of the most popular choices for people traveling and seeking housing. However, one problem exists. It is difficult for owners to come up with prices given the location and amenities. It is also hard to predict prices given seasons. Different features such as host ratings, season, location, and number of bedrooms make these two questions immensely complex. In this project, we try to provide insight into predicting prices given different neighborhoods in New York. The data set we picked comes from Inside Airbnb, which is an organization that periodically scrapes data from Airbnb listings. There are 12 columns in the data set. Four are descriptions of the host and 10 can serve as explanatory variables. The explanatory variables are: neighborhood, room_type, accommodation, bed, price, minimum nights, and number of reviews. The response variable is the rent price. Firstly, We will's start with exploratory analysis and data cleaning. Secondly, we'll use the frequentist approach to fit the data and analyze the models. Lastly, we'll describe how we are going to fit the model with respect to a Bayesian approach.   

We used R-Markdown to prepare this document. 

# 2. Exploratory Data Cleaning and Data Analysis


To start, we load our New York data from Inside Airbnb, and obtain the summary statistics on the numerical features. From table 1, there are signs of outliers which we will do further analysis to confirm. The two most notable ones are 42 beds and 1250 minimum amounts of nights. 

```{r,1, results='asis'}

data <- read.csv('nydata.csv')

numerical_features <- data[3:9]
categorical_features <- data[10:11]
price = summary(as.data.frame(data$price))
num_and_price <- data[c(3:9,12)]

```

```{r,echo=FALSE, results='asis'}
stargazer(num_and_price,type='latex', title = "Summary for Numerical Features", header = FALSE)
```

Next, we plot the density for each of the feature to get a sense of the shape. From the plots of each feature, there are signs of skewness from each of the features. The features are also poorly distributed. This also confirms our claim that there are outliers. The main outlier exist with minimum nights stay, price, and amount of beds. To remedy this, we will remove the outliers according to the summary statistics in Table 1. 


```{r, echo = FALSE, out.width = "50%", out.height = "50%"}
features <- c("latitude", "longitude", "accommodates", "beds", "minimum_nights", "number_of_reviews", "review_scores_rating")
for (i in 1:ncol(numerical_features)) {
  p <- ggplot(numerical_features, aes(x = numerical_features[[i]])) +
    geom_density(fill = "skyblue", color = "black") +
    labs(title = paste("Density Plot of Feature", features[i]), x = features[i], y = "Density") +
    theme_minimal()
  
  plot(p)  # Print each plot
}

```


We clean the data by using two filters. The first one is to only look at listing that is under 500 dollars. The second one is that we only look at listings that have a minimum nights of under 30. We choose these two because of the distribution plots above. Most data are clustered around price $<= 500$ and minimum nights $<= 30$. Looking at table 2, which is the summary stat for the cleaned data, and the density plots for the cleaned data, the skewness problem seems to be alleviated.  

```{r, echo=F,results='asis'}

#clean data
data_clean = subset(data,price <= 500 & minimum_nights <= 30)
numerical_features_clean <- data_clean[3:9]
categorical_features_clean <- data_clean[10:11]
price_clean <- data_clean$price

display <- data.frame(cbind(numerical_features_clean,price_clean))

stargazer(display,type='latex', title = "Figure 2: Summary Stat for Cleaned Numerical Features",header = FALSE)


```



```{r, echo = FALSE, out.width = "50%", out.height = "50%"}

#graph again 

for (i in 1:ncol(numerical_features_clean)) {
  p <- ggplot(numerical_features_clean, aes(x = numerical_features_clean[[i]])) +
    geom_density(fill = "skyblue", color = "black") +
    labs(title = paste("Density Plot of Feature", features[i]), x = features[i], y = "Density") +
    theme_minimal()
  
  print(p)  # Print each plot
}
```


Next, we check for correlation between the predictors. Because predictors such as number of reviews on a host might be correlated with other predictors that can reflect quality of housing , i.e number of beds and amenities, we want to make sure that our predictors does not show strong correlation. If our predictors are correlated, it can affect our model fit. To check, we created a correlation heat map of predictors. From figure 1 and 2, we see light correlation between some of the predictors. However, since no two predictor exhibit strong correlation, we will fit our model and analyze the model to see if our model is good fit. 

```{r, echo=F}
pairs(numerical_features_clean, main = "Figure 1: Pairs Plot")
cor_matrix <- cor(numerical_features_clean)
ggcorrplot(cor_matrix) + ggtitle("Figure 2: Correlation Heat Map")

```

To conclude our exploratory data analysis, we plot the type of housing in figure 3 and the relationship between neighborhood and price in table 3. From table 3, we break down the prices by the neighborhood. Note that the graph only shows 30 neighborhoods because there are around 200 total neighborhoods in New York. We will insert the total in the appendix.  

```{r, echo=F, results='asis',out.width="70%", out.height="70%"}
#graphs with the categorical features 

ggplot(categorical_features_clean, aes(x = categorical_features_clean$room_type)) +
  geom_bar(stat = "count", fill = "skyblue", color = "black") +
  labs(title = "Figure 4: Type of Housing Count Plot", x = "Category", y = "Count") +
  theme_minimal()

neighborhood_Price <- data_clean[11:12]
summary_price <- neighborhood_Price %>%
  group_by(neighbourhood_cleansed) %>%
  summarise(mean_price = mean(price),
            median_price = median(price),
            min_price = min(price),
            max_price = max(price),
            total_listings = n())

summary_price <- as.data.frame(summary_price[0:30,])

```

```{r, echo=FALSE,results='asis', out.width="70%", out.height="70%"}
stargazer(summary_price,type='latex', summary=F, title = "Summary Stat by Neighborhood", header = FALSE)
```

# 3. Frequentist Analysis

## 3.1 Proposed Frequentist Model(s)

We will be using Multiple Linear Regression to analyze our dataset. 

$$ Y_i = \beta_0 + \beta_1 X_{i1} + \dots + \beta_p X_{ij} + \epsilon_i$$

\begin{itemize}
  \item Y is the predicted rent price
  \item $\beta_0$ is the intercept
  \item $\beta_1, \dots, \beta_p$ are the coefficients for the respective $X_n$ independent variables
  \item $\epsilon_i$ is the error term
\end{itemize}

The variables that we included in our model were:

\begin{itemize}
  \item $area$, the New York neighborhood of the Airbnb listing
  \item $accommodates$, the number of individuals the Airbnb can accommodate
  \item $beds$, the number of beds in the Airbnb
  \item $bathrooms$, the number bathrooms in the Airbnb
  \item $min \ nights$, the minimum number of nights required to book the Airbnb
  \item $room \ type$, whether the Airbnb listing is a private room, shared room, hotel room, or the entire home/apartment
  \item $rating$, the rating score out of 5
\end{itemize}

I decided to only use categorical predictor, area, instead of longitude and latitude because it can give the same information on the data and makes analysis easier than numerical longitude and latitude.

## 3.2 Fitting the Frequentist Model(s)


We start by fitting the following 2 models without any interaction terms. 

The formula for the 2 models are as following:

\begin{itemize}
  \item $Price = \beta_0 + \beta_1 \times area + \beta_2 \times accommodates + \beta_3 \times bathrooms + \beta_4 {\times} nights + \beta_5 {\times } room \, type + \beta_6 \times beds + \beta_7 \times rating$
  \item $log(Price) = \beta_0 + \beta_1 \times area + \beta_2 \times accommodates + \beta_3 \times bathrooms + \beta_4 {\times} nights + \beta_5 {\times } room \, type + \beta_6 \times beds + \beta_7 \times rating$
\end{itemize}


```{r data, echo=F}
airbnb24 <- read.csv('ny_ny.24.csv')
airbnb24$rating[is.na(airbnb24$rating)] <- mean(airbnb24$rating, na.rm = TRUE)
airbnb24$logP <- log(airbnb24$price)
UES <- subset(airbnb24, area == "Upper East Side")
lmUES = lm(logP ~ log(accommodates) + bathrooms + log(min_nights) + room_type + beds, data=UES)
```


We fit the model by first testing if the model needed a log transformation. Thus, we graphed area vs. Price and area vs. log(Price).  We can see from figure 4 and 5 that after performing a log transformation on Price, the data becomes more linear and variance becomes more constant. This is further demonstrated if we graph the fitted values against the residuals in figure 5 and 6.

```{r}
lm24 = lm(price ~ as.factor(area) + accommodates + bathrooms + min_nights + 
            as.factor(room_type) + beds + rating, data=airbnb24)
log24 = lm(log(price) ~ as.factor(area) + accommodates + bathrooms + min_nights + 
             as.factor(room_type) + beds, data=airbnb24)
```

```{r, echo=F}
## Residuals vs. Y
ggplot(airbnb24, aes(x=as.factor(area), y=price)) + geom_point() +
  xlab("Area") + ylab("Price") + labs(title="Figure 4: Area vs. Price") + 
  theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1))
ggplot(airbnb24, aes(x=as.factor(area), y=log(price))) + geom_point() +
  xlab("Area") + ylab("Log of Price") + labs(title="Figure 5: Area vs. Log(Price)") + 
  theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1))
```



```{r, echo=F}
## Residuals vs. fitted values
ggplot(airbnb24, aes(x=lm24$fit, y=lm24$res)) + geom_point() +
  xlab("Fitted Values") + ylab("Residuals") + labs(title = "Figure 5: Without Log Transformation") +
  geom_hline(yintercept = 0, col=2)
ggplot(airbnb24, aes(x=log24$fit, y=log24$res)) + geom_point() +
  xlab("Log Fitted Values") + ylab("Residuals") + labs(title = "Figure 6: With Log Transformation") +
  geom_hline(yintercept = 0, col=2)
```

Thus, we decided to go with log(Price) in fitting our final model. 

Because there are two categorical variables in our dataset, we want to control for the interaction terms. To determine the best model, I manually use greedy backwards algorithm to find the most significant predictors between all order two interaction terms. 

The final model formula with most significant number of predictors is the following: 

$$ log(Price) = \beta_0 + \beta_1\times accommodates + \beta_2 \times bathrooms + \beta_3 \times min \ nights + \beta_4 \times as. room \ type \\ \beta_4 \times rating 
+ \beta_5 \times area {\times } bathrooms + \beta_7 \times room \ type \times accommodates  + \beta_8 \times rating + \beta_9 \times area \times rating$$

From figure 4 and 5 which contains the Anova table, and AIC/BIC values, we can see a general comparison between the models without a log transformation (lm24), the model with a log transformation (log24), and the final model we arrived at with a log transformation and interaction terms (lm1). From both figures and the adjusted $r^2$ value of 0.438 , the model that includes interaction terms and log transform performs the best.


```{r 2024 fit}
lm1 = lm(log(price) ~ (accommodates) + (min_nights)  + (rating)+ (bathrooms) 
+ rating + as.factor(room_type) +as.factor(area)*(bathrooms) 
+ as.factor(room_type)*(accommodates), data=airbnb24)
```

```{r, echo=F, results='asis'}
mod1 <- anova(lm1,log24)
stargazer(mod1,summary = FALSE, type = "latex", title = "Anova for 2 models", header = FALSE)
```
```{r,echo=F, results='asis'}
aic_bic_df <- data.frame(
  Name = c("Log Interaction Model", "Log Linear Model","Linear Model"),
  AIC = c(AIC(lm1), AIC(log24), AIC(lm24)), 
  BIC = c(BIC(lm1), BIC(log24), BIC(lm24))  
)

stargazer(aic_bic_df,summary = FALSE, type = "latex",title = "Figure 12: AIC/BIC for 3 Models",header = FALSE)

```

```{r, echo=FALSE}
predicted_values <- predict(lm1)

# Mean of predicted values
mean_predicted <- mean(predicted_values)
sd_predicted <- sd(predicted_values)

airbnb24$predicted <- predicted_values

means_by_area <- aggregate(predicted ~ area, data = airbnb24, FUN = mean)
sds_by_area <- aggregate(predicted ~ area, data = airbnb24, FUN = sd)

means_sds_by_area <- merge(means_by_area, sds_by_area, by = "area")
colnames(means_sds_by_area) <- c("area", "Log predicted_mean", "Log predicted_sd")


```

Given that there are 32 neighborhoods in our sample, we will not display the final equation of the fitted model as there are 288 coefficients, but we will briefly explain our results below.

In our final fitted model, the intercept $\beta_0$ = 3.857386, which means if the Airbnb could accommodate 0 people, had 0 beds and bathrooms, required 0 minimum nights to book, was the entire home or apartment, and was located in Battery Park, the estimated rent price of the Airbnb is ~$50. We also found that the rent price of the Airbnb increases when it can accommodate more people, has more bathrooms, or is a private room and decreases the more minimum nights are required to book, is a hotel room or shared room, or has more beds. 

As for the neighborhoods, we found that, compared to Battery Park, the neighborhoods that increased the rent price of an Airbnb the greatest were Civic Center, Inwood, Morningside Heights, and Roosevelt Island, and the neighborhoods that decreased the rent price the greatest were Flatiron District, Grammercy, and Stuyvesant Town. 

The mean predicated log price is around 5.2 with a standard deviation of 0.539074. In table 6, I also plot the first 30 neighborhoods' mean and standard deivation to get an idea of how spread out the price is.  


```{r echo=FALSE, results='asis', header=FALSE}
stargazer(means_sds_by_area[1:30,],summary = FALSE, type = "latex",title = "Mean and SD by neighborhoods", header = FALSE) 
```


# 4. Bayesian Analysis

## 4.1 Proposed Bayesian Model(s)

In the Bayesian context, we can use our Airbnb data in two ways. We can use a none-informative prior such as $1/\sigma^2$ that does follows $IG(0,0)$. Alternatively, we can use this dataset as the prior and download the more recent 2024 as our data. For this proposal, we will use the reference prior, $1/\sigma^2$ due to the new data have not been published as the proposal due date. 

We now formally define the notion for this section: 

$$Y_{n{\times}1} = X_{n{\times}p}\beta_{p{\times}1}+\epsilon_{n{\times}1}$$

\begin{itemize}
  \item $X_{n{\times}p}$ = data matrix 
  \item $Y_{n{\times}1}$ = price response vector
  \item $\beta_{p{\times}1}$ = predictor vector
  \item $\epsilon_{n{\times}1}$ ~ $N(0,\sigma^2I_{p{\times}p})$ = noise vector 
\end{itemize}

## 4.2 Fitting the Bayesian model(s)

Following the correlation heat map from exploratory data analysis, we see that some predictors are correlated. However, since the correlation are not over 0.5, we can assume that it will not a significant impact and each predictors are independent. 

To start the Bayesian regression analysis, we will center data first by subtracting the mean from each column. 

```{r,results='asis'}
means = colMeans(numerical_features_clean, na.rm = TRUE) 
numerical_features_clean_substracted <- numerical_features_clean
for (i in seq_along(df)) {
  numerical_features_clean_substracted[, i] <- ifelse(is.na(numerical_features_clean[, i]), NA, numerical_features_clean[, i] - means[i])
}

stargazer(numerical_features_clean_substracted,type = "latex",title = "Summary of Centered Data", header=F)
```

Since this a multiple linear regression. We can define the distribution as the following: 


\begin{itemize}
  \item $\pi(\beta\ | \sigma^2, X,Y)$ ~ $N(\hat{\beta}, \sigma^2(X{'}X)^{-1}$ where $\hat{\beta}=(X{'}X)^{-1}X'Y$
  \item $\pi(\sigma^2|X,Y)$ ~. $IG((n-p)/2,SSE/2)$ where $SSE = $ ~ $(Y-X\hat{\beta})'(Y-X\hat{\beta})$
\end{itemize}



Alternatively, we can fit a linear hierarchical model to better compensate for neighborhood data. To do this, we would use the brms library to do the following. 

Stage 2 

$\alpha | \beta, \Sigma_{\alpha} \sim \text{MVN}_2 (\beta, \Sigma_{\alpha})$

Stage 3 
\begin{itemize}
  \item $\beta | \mu, \Sigma_0 \sim \text{MVN}_2 (\mu, \Sigma_0)$
  \item $\Sigma_{\alpha} \sim \text{Inv-Wishart}(R, \rho)$
  \item $\sigma_e^2 \sim \text{IG}(a_e, b_e)$
\end{itemize}

We'll be able to get a better understanding of to do this after 12/14/24 lectures and learning how to use the brms after the lab06. 


To perform sensitivity test, we will make sure the following conditions are satisfied. 
\begin{itemize}
  \item $n > p$ 
  \item Making sure the predictors are independent
  \item $X'X$ is convertible.
  \item Checking collinearity
  \item $SSE > 0$
\end{itemize}

Another way that we can check for sensitivity is using data from prior years to do cross validation to make sure our model is correct. 


For ensuring MCMC convergence, we will plot and monitor the following showing signs of convergence:
\begin{itemize}
  \item trace plots
  \item histograms 
  \item Gelman-Rubin convergence diagnosis
  \item Effective sample size
\end{itemize}

## 4.3 Prediction

After generating the posterior distribution, we can generate the predictive posterior distribution, and compute the predictive mean and credible intervals.     

# 5. Discussion

<!-- In this section, discuss how you can improve your model. -->

We can improve our model by narrowing down the neighborhoods where we do not have enough information in order to make reliable predictions and remove it from our model. While this means that we will have to exclude predictions for that area, that could potentially help in obtaining a better fitted model. Another way we can improve our model is to use a informative prior derived from previous years' dataset. However, this might be difficult because of other variable impacting the Airbnb market such as the economy. 

# 6. Contributions

<!-- In this section, discuss the percentage of your contributions to the development final project proposal. Report the number of hours you have worked on the proposal, and the sections you are involved. -->

<!-- Please also discuss briefly the contributions of your teammate(s), as well as the help and support you got from your teammates(s). -->

For the first proposal, Bridgette and Jake split evenly. Bridgette did sections 3,4,and 5. Jake did selection 1 and 2. For the final proposal, Jake did the revision. We have spent 15+ hours for this proposal

# References

<div id="refs"></div>

# Appendix

**From Section 2:**

```{r, results='hide', fig.show='hide'}
summary_price <- as.data.frame(summary_price)

data <- read.csv('nydata.csv')

numerical_features <- data[3:9]
categorical_features <- data[10:11]
price <- data$price
summary(price)
summary(numerical_features)

features <- c("latitude", "longitude", "accommodates", "beds", "minimum_nights", "number_of_reviews", "review_scores_rating")

for (i in 1:ncol(numerical_features)) {
  p <- ggplot(numerical_features, aes(x = numerical_features[[i]])) +
    geom_density(fill = "skyblue", color = "black") +
    labs(title = paste("Density Plot of Feature", features[i]), x = features[i], y = "Density") +
    theme_minimal()
  
  plot(p)  # Print each plot
}

#clean data
data_clean = subset(data,price <= 500 & minimum_nights <= 30)
numerical_features_clean <- data_clean[3:9]
categorical_features_clean <- data_clean[10:11]
price_clean <- data_clean$price

summary(price_clean)
summary(numerical_features_clean)

#graph again 
for (i in 1:ncol(numerical_features_clean)) {
  p <- ggplot(numerical_features_clean, aes(x = numerical_features_clean[[i]])) +
    geom_density(fill = "skyblue", color = "black") +
    labs(title = paste("Density Plot of Feature", features[i]), x = features[i], y = "Density") +
    theme_minimal()
  
  print(p)  # Print each plot
}
plot(price_clean)

pairs(numerical_features_clean)
cor_matrix <- cor(numerical_features_clean)
cor_matrix
ggcorrplot(cor_matrix)

#graphs with the categorical features 
ggplot(categorical_features_clean, aes(x = categorical_features_clean$room_type)) +
  geom_bar(stat = "count", fill = "skyblue", color = "black") +
  labs(title = "Count Plot", x = "Category", y = "Count") +
  theme_minimal()

neighborhood_Price <- data_clean[11:12]
summary_price <- neighborhood_Price %>%
  group_by(neighbourhood_cleansed) %>%
  summarise(mean_price = mean(price),
            median_price = median(price),
            min_price = min(price),
            max_price = max(price),
            total_listings = n())

summary_price
```

**From Section 3.2**

```{r, results='hide', fig.show='hide'}
airbnb24 <- read.csv('ny_ny.24.csv')
airbnb24$logP <- log(airbnb24$price)

lm24 = lm(price ~ area + accommodates + bathrooms + min_nights + room_type + beds, data=airbnb24)
log24 = lm(logP ~ area + accommodates + bathrooms + min_nights + room_type + beds, data=airbnb24)

lm1 = lm(logP ~ area*log(accommodates) + area*bathrooms + area*log(min_nights) + area*room_type + area*beds, data=airbnb24)

## Residuals vs. Y
ggplot(airbnb24, aes(x=area, y=price)) + geom_point() +
  xlab("Area") + ylab("Price") + labs(title="Area vs. Price")
ggplot(airbnb24, aes(x=area, y=logP)) + geom_point() +
  xlab("Area") + ylab("Log of Price") + labs(title="Area vs. Log(Price)")

## Residuals vs. fitted values
ggplot(airbnb24, aes(x=lm24$fit, y=lm24$res)) + geom_point() +
  xlab("Fitted Values") + ylab("Residuals") + labs(title = "Without Log Transformation") +
  geom_hline(yintercept = 0, col=2)
ggplot(airbnb24, aes(x=log24$fit, y=log24$res)) + geom_point() +
  xlab("Log Fitted Values") + ylab("Residuals") + labs(title = "With Log Transformation") +
  geom_hline(yintercept = 0, col=2)

summary(lm1)$r.squared
summary(log24)$r.squared
summary(lm24)$r.squared

coef(lm1)[1]  #log(price)
tidy(lm1)
tidy(lmUES)
```